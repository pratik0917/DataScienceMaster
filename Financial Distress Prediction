#Task 1 -  Load the dataset and  split into train and test sets
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')


# Path variable

df = pd.read_csv(path)
df.head(5)
df.drop("Unnamed: 0",axis=1,inplace=True)
y=df['SeriousDlqin2yrs']
X=df.drop('SeriousDlqin2yrs',axis=1)
count=df['SeriousDlqin2yrs'].value_counts()
print(count)
X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.3 , random_state = 6 )


# First 5 columns


# Independent variables


# Dependent variable


# Check the value counts


# Split the data set into train and test sets


#Task 2 - Plot a scatter plot for all the independent variables with the dependent variable
# save list of all the columns of X in cols
cols=list(X_train.columns)
print(cols)

# create subplots
fig ,axes = plt.subplots(nrows = 5 , ncols = 2,figsize=(10,5))

# nested for loops to iterate over all the features and plot the same



#Task 3 - Check for missing values and replace it with appropriate values
# Check for null values
print(X_train.isnull().sum())

# Filling the missing values for columns in training data set
X_train['MonthlyIncome']=X_train['MonthlyIncome'].fillna(X_train['MonthlyIncome'].median())
X_test['MonthlyIncome']=X_test['MonthlyIncome'].fillna(X_test['MonthlyIncome'].median())

# Filling the missing values for columns in testing data set
X_train['NumberOfDependents']=X_train['NumberOfDependents'].fillna(X_train['NumberOfDependents'].median())
X_test['NumberOfDependents']=X_test['NumberOfDependents'].fillna(X_test['NumberOfDependents'].median())



# Filling the missing values for columns in testing data set
print(X_train.isnull().sum())

# Checking for null values


#Task 4 - Feature selection
# Correlation matrix for training set
corr= X_train.corr()

# Plot the heatmap of the correlation matrix
X_train.drop(['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse'],axis=1,inplace=True)
X_test.drop(['NumberOfTime30-59DaysPastDueNotWorse','NumberOfTime60-89DaysPastDueNotWorse'],axis=1,inplace=True)

# drop the columns which are correlated amongst each other except one


#Task 5 - Scaling the features 
from sklearn.preprocessing import StandardScaler

scaler=StandardScaler()
co=list(X_train.columns)
print(X_train.head(3))
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)
print(X_train)

#Task 6 - Predict the values after building a Machine learning model
# Import Logistic regression model and accuracy score
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression

# Instantiate the model in a variable in log_reg
log_reg= LogisticRegression()
log_reg.fit(X_train,y_train)
y_pred=log_reg.predict(X_test)
accuracy=accuracy_score(y_test,y_pred)



# Fit the model on training data


# Predictions of the training dataset


# accuracy score


#Task 7 - Is our prediction right?
# Import all the models
from sklearn.metrics import roc_auc_score
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.metrics import f1_score, confusion_matrix, classification_report
from sklearn.metrics import precision_score, recall_score
# Plot the auc-roc curve
score=roc_auc_score(y_test,y_pred)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)


f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))


plt.plot(fpr,tpr,label="Logistic model, auc="+str(auc))
# Evaluation parameters for the model


#Task 8 - Balancing the dataset
# Import SMOTE from imblearn library
from imblearn.over_sampling import SMOTE

# Check value counts of target variable for data imbalance
smote=SMOTE(random_state=9)
X_sample ,y_sample=smote.fit_sample(X_train,y_train)

# Instantiate smote


# Fit Smote on training set

# Check for count of class



#Task 9 - Effect of applying SMOTE?
# Fit logistic regresion model on X_sample and y_sample


# Store the result predicted in y_pred

# Store the auc_roc score


# Store the probablity of any class


# Plot the auc_roc_graph


# Print f1_score,Precision_score,recall_score,roc_auc_score and confusion matrix
log_reg= LogisticRegression()
log_reg.fit(X_sample,y_sample)
y_pred=log_reg.predict(X_test)
score=roc_auc_score(y_test,y_pred)
y_pred_proba = log_reg.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)


f1 = f1_score(y_test, log_reg.predict(X_test))
precision = precision_score(y_test, log_reg.predict(X_test))
recall = recall_score(y_test, log_reg.predict(X_test))
roc_auc = roc_auc_score(y_test, log_reg.predict(X_test))


plt.plot(fpr,tpr,label="Logistic model, auc="+str(auc))

# Import RandomForestClassifier from sklearn library
from sklearn.ensemble import RandomForestClassifier

# Instantiate RandomForrestClassifier to a variable rf.
rf = RandomForestClassifier(random_state=9)

# Fit the model on training data.
rf.fit(X_sample, y_sample)

# store the predicted values of testing data in variable y_pred.
y_pred = rf.predict(X_test)

# Store the different evaluation values.

f1 = f1_score(y_test, rf.predict(X_test))
precison = precision_score(y_test, rf.predict(X_test))
recall = recall_score(y_test, rf.predict(X_test))
score = roc_auc_score(y_test, rf.predict(X_test))
print ('Confusion_matrix' + '\n',confusion_matrix(y_test, rf.predict(X_test)))
print ('Classification_report' + '\n' + classification_report(y_test,y_pred))

# Plot the auc_roc graph
score = roc_auc_score(y_pred , y_test)
y_pred_proba = rf.predict_proba(X_test)[:,1]
fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)
auc = metrics.roc_auc_score(y_test, y_pred_proba)
plt.plot(fpr,tpr,label="Random_forest model, auc="+str(auc))
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc=4)
plt.show()
